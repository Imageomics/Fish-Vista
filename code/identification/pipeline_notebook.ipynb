{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8d64ad-7d75-47f9-b085-7be3939377c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ksmehrab/miniconda/envs/data_env/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2177b897-6a3b-4a8a-ba7f-3e8311368d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup identification dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class FishAirDatasetProcessed(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_file:Path,\n",
    "        img_dir:Path,\n",
    "        transform,\n",
    "        traits_to_detect = ['adipose_fin', 'pelvic_fin', 'barbel', 'multiple_dorsal_fin']\n",
    "    ):\n",
    "        self.data_file = data_file\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        if str(self.data_file).endswith('csv'):\n",
    "            self.df = pd.read_csv(data_file)\n",
    "        self.traits_to_detect = traits_to_detect\n",
    "        self.num_classes = len(self.traits_to_detect)\n",
    "        self.samples = self.df.to_dict('records') # List of samples, where each each sample is a dict \n",
    "        # adipose_fin,pelvic_fin,barbel,multiple_dorsal_fin\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        sample is a dict with keys: ARKID, original_filename, arkid_filename, species_name, label \n",
    "        \"\"\"\n",
    "        sample = self.samples[idx] \n",
    "        \n",
    "        filename = sample['filename']\n",
    "\n",
    "        sample_img_dir = self.img_dir\n",
    "        img_path = sample_img_dir / filename\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        img = self.transform(img) # torchvision transform\n",
    "        label = [float(sample[t]) for t in self.traits_to_detect]\n",
    "        label = np.array(label)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def get_img_filenames(self, indices):\n",
    "        return [self.samples[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b692c2-c096-4d07-9dc8-82081b514800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file uses torchvision pretrained models, but modifies the final fc layer to the number of classes\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "from torchvision.models import vgg19_bn, VGG19_BN_Weights\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision.models import swin_b, Swin_B_Weights\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "\n",
    "def get_custom_model(\n",
    "    model_name:str,\n",
    "    num_classes:int,\n",
    "    pretrained:bool=True\n",
    "):\n",
    "    if model_name == 'resnet18':\n",
    "        if pretrained:\n",
    "            weights = ResNet18_Weights.DEFAULT\n",
    "            model = resnet18(weights=weights)\n",
    "            model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = resnet18()\n",
    "            model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'resnet34':\n",
    "        if pretrained:\n",
    "                weights = ResNet34_Weights.DEFAULT\n",
    "                model = resnet34(weights=weights)\n",
    "                model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = resnet34()\n",
    "            model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "    elif model_name == \"resnet50\":\n",
    "        if pretrained:\n",
    "                weights = ResNet50_Weights.DEFAULT\n",
    "                model = resnet50(weights=weights)\n",
    "                model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = resnet50()\n",
    "            model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'vit_b_32':\n",
    "        if pretrained:\n",
    "            weights = ViT_B_32_Weights.DEFAULT\n",
    "            model = vit_b_32(weights=weights)\n",
    "            model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = vit_b_32()\n",
    "            model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'vit_b_16':\n",
    "        if pretrained:\n",
    "            weights = ViT_B_16_Weights.DEFAULT\n",
    "            model = vit_b_16(weights=weights)\n",
    "            model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = vit_b_32()\n",
    "            model.heads = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'vgg19':\n",
    "        if pretrained:\n",
    "            weights = VGG19_BN_Weights.DEFAULT\n",
    "            model = vgg19_bn(weights=weights)\n",
    "            model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = vgg19_bn()\n",
    "            model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'swin_b':\n",
    "        if pretrained:\n",
    "            weights = Swin_B_Weights.DEFAULT\n",
    "            model = swin_b(weights=weights)\n",
    "            model.head = nn.Linear(in_features=1024, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = swin_b()\n",
    "            model.head = nn.Linear(in_features=1024, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'inception_v3':\n",
    "        if pretrained:\n",
    "            weights = Inception_V3_Weights.DEFAULT\n",
    "            model = inception_v3(weights=weights)\n",
    "            model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            model = inception_v3()\n",
    "            model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a84600-12de-4c71-8da0-d3338da810d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR]\n",
      "                             [--model {resnet34,resnet18,resnet50,vit_b_32,vgg19,swin_b,inception_v3}]\n",
      "                             [--batch-size BATCH_SIZE] [--epoch EPOCH]\n",
      "                             [--seed SEED] --dataset\n",
      "                             {fishair130-bal-50,fishair130-imb-low50,fishair130-overs-500,fishair_processed}\n",
      "                             [--decay DECAY] [--no-augment] [--name NAME]\n",
      "                             [--resume] [--net_t NET_T]\n",
      "                             [--focal_gamma FOCAL_GAMMA]\n",
      "                             [--loss_type {CE,Focal}] [--wandb]\n",
      "                             [--cosine_annealing] [--num_workers NUM_WORKERS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataset\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmehrab/miniconda/envs/data_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Setup config.py\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "\n",
    "from data_setup import get_transform, get_dataset_and_dataloader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    N_GPUS = torch.cuda.device_count()\n",
    "else:\n",
    "    N_GPUS = 0\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Trait Identification Pipeline')\n",
    "    parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "    parser.add_argument('--model', default='resnet18', type=str, choices=['resnet34', 'resnet18', 'resnet50', 'vit_b_32', 'vgg19', 'swin_b', 'inception_v3'], help='model type (default: ResNet18)')\n",
    "    parser.add_argument('--batch-size', default=128, type=int, help='batch size')\n",
    "    parser.add_argument('--epoch', default=200, type=int,\n",
    "                        help='total epochs to run')\n",
    "    parser.add_argument('--output_path', default=None, type=str,\n",
    "                        help='path to save all outputs')\n",
    "    parser.add_argument('--seed', default=None, type=int, help='random seed')\n",
    "    parser.add_argument('--dataset', required=True,\n",
    "                        choices=['fishair130-bal-50', 'fishair130-imb-low50', 'fishair130-overs-500', 'fishair_processed'], help='Dataset')\n",
    "    parser.add_argument('--decay', default=2e-4, type=float, help='weight decay')\n",
    "    parser.add_argument('--no-augment', dest='augment', action='store_false',\n",
    "                        help='use standard augmentation (default: True)')\n",
    "\n",
    "    parser.add_argument('--name', default='0', type=str, help='name of experiment or run')\n",
    "    parser.add_argument('--resume', '-r', action='store_true',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--checkpoint_path', default=None, type=str,\n",
    "                        help='checkpoint path of network for train')\n",
    "\n",
    "    parser.add_argument('--focal_gamma', default=1.0, type=float, help='Hyper-parameter for Focal Loss')\n",
    "\n",
    "    parser.add_argument('--loss_type', default='BCE', type=str,\n",
    "                        choices=['BCE', 'WBCE', 'Focal'],\n",
    "                        help='Type of loss for imbalance')\n",
    "\n",
    "    parser.add_argument('--wandb', action='store_true', help='wandb logging')\n",
    "    parser.add_argument('--cosine_annealing', action='store_true', help='Use cosine annealing')\n",
    "    parser.add_argument('--num_workers', default=1, type=int, help='Number of dataloader workers')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "ARGS = parse_args()\n",
    "if ARGS.seed is not None:\n",
    "    SEED = ARGS.seed\n",
    "else:\n",
    "    SEED = np.random.randint(10000)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATASET = ARGS.dataset\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "MODEL = ARGS.model\n",
    "WANDB = ARGS.wandb\n",
    "\n",
    "LR = ARGS.lr\n",
    "EPOCH = ARGS.epoch\n",
    "NUM_WORKERS = ARGS.num_workers\n",
    "START_EPOCH = 0\n",
    "\n",
    "# Setup logging \n",
    "if os.path.exists(ARGS.output_path):\n",
    "    print(f\"Output path: {ARGS.output_path} exists. Continuing will overwrite results. Continue? [Y/n]\")\n",
    "    c = input()\n",
    "    if c == 'Y':\n",
    "        pass\n",
    "    elif c == 'n':\n",
    "        print('Quitting execution')\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print('Invalid selection. Quitting execution')\n",
    "        sys.exit()\n",
    "else:\n",
    "    os.mkdir(path, exist_ok=True)\n",
    "\n",
    "BASE_FILENAME = f\"S{SEED}_{ARGS.name}_\" \\\n",
    "    f\"{DATASET}_{MODEL}\"\n",
    "\n",
    "if WANDB:\n",
    "    os.environ['WANDB_DIR'] = str(BASE_FILENAME)\n",
    "    wandb.init(project=BASE_FILENAME)\n",
    "    config = {\n",
    "        \"model_name\": ARGS.name,\n",
    "        \"learning_rate\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCH,\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    \n",
    "# Data\n",
    "print('==> Preparing data: %s' % DATASET)\n",
    "if DATASET == 'fishair_processed':\n",
    "    mean = torch.tensor([0.9353, 0.9175, 0.8923])\n",
    "    std = torch.tensor([0.1535, 0.1933, 0.2464])\n",
    "    transform = get_transform(224, mean, std, 'squarepad_augment_normalize')\n",
    "    test_transform = get_transform(224, mean, std, 'squarepad_no_augment_normalize')\n",
    "    train_file = Path('/data/DatasetTrackFinalData/Identification/trait_identification_train.csv')\n",
    "    val_file = Path('/data/DatasetTrackFinalData/Identification/trait_identification_val.csv')\n",
    "    test_file = Path('/data/DatasetTrackFinalData/Identification/trait_identification_test_filtered.csv')\n",
    "    lv_sp_normal_test_file = Path('/data/DatasetTrackFinalData/Identification/trait_identification_test_leave_out_filtered.csv')\n",
    "    lv_sp_difficult_test_file = Path('/data/DatasetTrackFinalData/Identification/trait_identification_test_leave_out_difficult.csv')\n",
    "    img_dir = Path('/data/BGRemovedCropped/all')\n",
    "else:    \n",
    "    raise NotImplementedError('Dataset not implemented')\n",
    "\n",
    "train_dataset, train_loader = get_dataset_and_dataloader(\n",
    "    data_file=train_file,\n",
    "    img_dir=img_dir,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=ARGS.num_workers\n",
    ")\n",
    "\n",
    "val_dataset, val_loader = get_dataset_and_dataloader(\n",
    "    data_file=val_file,\n",
    "    img_dir=img_dir,\n",
    "    transform=test_transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=ARGS.num_workers\n",
    ")\n",
    "\n",
    "test_dataset, test_loader = get_dataset_and_dataloader(\n",
    "    data_file=test_file,\n",
    "    img_dir=img_dir,\n",
    "    transform=test_transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=ARGS.num_workers\n",
    ")\n",
    "\n",
    "if lv_sp_normal_test_file:\n",
    "    lv_sp_normal_dataset, lv_sp_normal_loader = get_dataset_and_dataloader(\n",
    "        data_file=lv_sp_normal_test_file,\n",
    "        img_dir=img_dir,\n",
    "        transform=test_transform,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=ARGS.num_workers\n",
    "    )\n",
    "\n",
    "if lv_sp_difficult_test_file:\n",
    "    lv_sp_dif_dataset, lv_sp_dif_loader = get_dataset_and_dataloader(\n",
    "        data_file=lv_sp_difficult_test_file,\n",
    "        img_dir=img_dir,\n",
    "        transform=test_transform,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=ARGS.num_workers\n",
    "    )\n",
    "\n",
    "TRAITS_TO_DETECT = train_dataset.traits_to_detect\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr_init, epoch, scheduler):\n",
    "    lr = lr_init\n",
    "    if epoch < 5:\n",
    "        lr = (epoch + 1) * lr_init / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        if ARGS.cosine_annealing:\n",
    "            assert scheduler != None, \"Scheduler cannot be None if cosine annealing is set\"\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate(net, dataloader, epoch, type):\n",
    "    is_training = net.training\n",
    "    net.eval()\n",
    "    criterion = nn.BCEWithLogitsLoss(reduce=True, reduction='mean')\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    all_predicted = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        batch_size = inputs.size(0)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        breakpoint()\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "        predicted = torch.sigmoid(outputs) # Get probabilities\n",
    "        \n",
    "        all_predicted.append(predicted.cpu().detach().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        total += batch_size\n",
    "\n",
    "    all_predicted = np.vstack(all_predicted)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    average_precisions = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        ap = average_precision_score(all_targets[:, i], all_predicted[:, i])\n",
    "        average_precisions.append(ap)\n",
    "        print(f'{TRAITS_TO_DETECT[i]} - AP: {ap}')\n",
    "\n",
    "    mean_ap = np.mean(average_precisions)\n",
    "    print(f'Mean Average Precision: {mean_ap}')\n",
    "\n",
    "    all_preds_threshold = all_predicted >= 0.50\n",
    "    all_preds_threshold = all_preds_threshold.astype(int)\n",
    "\n",
    "    f1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        f1 = f1_score(all_targets, all_preds_threshold[:, i], average='macro')\n",
    "        f1s.append(f1)\n",
    "        precision = precision_score(all_targets, all_preds_threshold[:, i], average='macro')\n",
    "        precisions.append(precision)\n",
    "        recall = recall_score(all_targets, all_preds_threshold[:, i], average='macro')\n",
    "        recalls.append(recall)\n",
    "        \n",
    "\n",
    "    results = {\n",
    "        'loss': total_loss / (total + eps),\n",
    "        'aps': average_precisions,\n",
    "        'map': mean_ap,\n",
    "        'f1_score': f1s,\n",
    "        'precision': precisions,\n",
    "        'recall': recalls\n",
    "    }\n",
    "\n",
    "    msg = f\"{type} | Epoch: {epoch}/{EPOCH} | Loss: {total_loss / (total + eps)} | AP: {average_precisions} | MAP: {map} | f1: {f1s} | Precs: {precisions} | Recs: {recalls}\"\n",
    "    \n",
    "    print(msg)\n",
    "\n",
    "    net.train(is_training)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9eb08-2b0e-4b97-9cb0-5b7c7cdcab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "from config import *\n",
    "\n",
    "from model import get_custom_model\n",
    "\n",
    "def train_epoch(model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    all_predicted = []\n",
    "    all_targets = []\n",
    "    for inputs, targets in tqdm(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        # outputs, _ = model(normalizer(inputs))\n",
    "        if 'inception' in MODEL:\n",
    "            outputs, _ = model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        breakpoint()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "        total += batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.sigmoid(outputs) # Get probabilities\n",
    "        all_predicted.append(predicted.cpu().detach().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    all_predicted = np.vstack(all_predicted)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    average_precisions = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        ap = average_precision_score(all_targets[:, i], all_predicted[:, i])\n",
    "        average_precisions.append(ap)\n",
    "        print(f'{TRAITS_TO_DETECT[i]} - AP: {ap}')\n",
    "\n",
    "    mean_ap = np.mean(average_precisions)\n",
    "    \n",
    "    eps = 0.000001\n",
    "    msg = f\"Training Loss: {train_loss / (total + eps)} | Training AP: {average_precisions} | Training  MAP: {map}\"\n",
    "    print(msg)\n",
    "\n",
    "    return train_loss / (total + eps), map\n",
    "\n",
    "def save_checkpoint(map, model, optim, epoch, index=False, checkpoint_stats=None):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "\n",
    "    state = {\n",
    "        'net': model.state_dict(),\n",
    "        'optimizer': optim.state_dict(),\n",
    "        'map': map,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state()\n",
    "    }\n",
    "\n",
    "    if checkpoint_stats:\n",
    "        for k, v in checkpoint_stats.items():\n",
    "            state[k] = v\n",
    "\n",
    "    if index:\n",
    "        ckpt_name = 'ckpt_epoch' + str(epoch) + '_' + str(SEED) + '.t7'\n",
    "    else:\n",
    "        ckpt_name = 'ckpt_' + str(SEED) + '_' + str(BASE_FILENAME) + '.t7'\n",
    "    # ARGS.output_path\n",
    "    ckpt_path = os.path.join(ARGS.output_path, ckpt_name)\n",
    "    torch.save(state, ckpt_path)\n",
    "\n",
    "def save_checkpoint_stats_json(checkpoint_stats, epoch, index=True, type='test'):\n",
    "    checkpoint_stats['epoch'] = epoch\n",
    "    if index:\n",
    "        ckpt_stat_name = 'ckpt_stats_' + type + \"_\" + BASE_FILENAME + \"_\" + str(epoch) + '.json'\n",
    "    else:\n",
    "        ckpt_stat_name = 'ckpt_stats_' + type + \"_\" + BASE_FILENAME + \"_\" + '.json'\n",
    "    ckpt_stat_path = os.path.join(ARGS.output_path, ckpt_stat_name)\n",
    "    with open(ckpt_stat_path, 'w') as f:\n",
    "        json.dump(checkpoint_stats, f)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "BEST_VAL = 0  # best validation accuracy\n",
    "\n",
    "print(f\"==> Building model from custom model: {MODEL}\")\n",
    "\n",
    "# Setup Model\n",
    "model = get_custom_model(\n",
    "    model_name=MODEL,\n",
    "    num_classes=N_CLASSES,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = optim.SGD(net.parameters(), lr=ARGS.lr, momentum=0.9, weight_decay=ARGS.decay)\n",
    "    if ARGS.cosine_annealing:\n",
    "        scheduler =torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "# Check if we need to start from checkpoint\n",
    "if ARGS.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "\n",
    "    if ARGS.checkpoint_path is not None:\n",
    "        ckpt_t = torch.load(ARGS.checkpoint_path)\n",
    "        model.load_state_dict(ckpt_t['net'])\n",
    "        optimizer.load_state_dict(ckpt_t['optimizer'])\n",
    "        START_EPOCH = ckpt_t['epoch'] + 1\n",
    "\n",
    "# Set loss function\n",
    "if ARGS.loss_type == 'BCE':\n",
    "    criterion = nn.BCEWithLogitsLoss(reduce=True, reduction='mean').to(device)\n",
    "elif ARGS.loss_type == 'WBCE':\n",
    "    pos_weight = get_pos_weight(train_file)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduce=True, reduction='mean').to(device)\n",
    "else:\n",
    "    raise NotImplementedError(\"Loss not implemented\")\n",
    "    \n",
    "# Training loop\n",
    "for epoch in range(START_EPOCH, EPOCH):\n",
    "    logger.log(' * Epoch %d: %s' % (epoch, LOGDIR))\n",
    "    \n",
    "    # Adjust learning rate:\n",
    "    #   epoch < 5: linear warmup of learning rate\n",
    "    #   epoch > 5 and cosine_annealing: cosine annealing scheduler\n",
    "    #   epoch > 5 and no cosine_annealing: reduce lr linearly beyond epoch 160 and 180\n",
    "    #       Following LDAM-DRW\n",
    "    adjust_learning_rate(optimizer, LR, epoch, scheduler)\n",
    "    \n",
    "    train_loss, train_map = train_epoch(\n",
    "        model=model,\n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader\n",
    "    )\n",
    "    train_stats = {'train_loss': train_loss, 'train_map': train_map}\n",
    "\n",
    "    # Validate\n",
    "    val_eval = evaluate(\n",
    "        net=model,\n",
    "        dataloader=val_loader,\n",
    "        type='Val',\n",
    "        epoch=epoch\n",
    "    )\n",
    "    \n",
    "    val_map = val_eval['map']\n",
    "    if val_map >= BEST_VAL:\n",
    "        BEST_VAL = val_map\n",
    "        # test_loader, lv_sp_normal_loader, lv_sp_dif_loader, lv_sp_normal_test_file, lv_sp_difficult_test_file\n",
    "        checkpoint_stats_k =  ['loss', 'aps', 'map', 'f1_score', 'precision', 'recall']\n",
    "\n",
    "        def _convert_scalar(x):\n",
    "            if hasattr(x, 'item'):\n",
    "                x = x.item()\n",
    "            return x\n",
    "\n",
    "        test_stats = evaluate(\n",
    "            net=model,\n",
    "            dataloader=test_loader,\n",
    "            type='Test',\n",
    "            epoch=epoch\n",
    "        )\n",
    "        \n",
    "        test_stats = {k: _convert_scalar(v) for k, v in test_stats.items() if k in checkpoint_stats_k}\n",
    "        \n",
    "        all_test_stats = {\n",
    "            'normal_test_stats': test_stats\n",
    "        }\n",
    "\n",
    "        if lv_sp_normal_test_file:\n",
    "            lv_sp_normal_test_stats = evaluate(\n",
    "                net=model,\n",
    "                dataloader=lv_sp_normal_loader,\n",
    "                type='Test Lv Sp Normal',\n",
    "                epoch=epoch\n",
    "            )\n",
    "            lv_sp_normal_test_stats = {k: _convert_scalar(v) for k, v in lv_sp_normal_test_stats.items() if k in checkpoint_stats_k}\n",
    "            all_test_stats['leave_sp_normal_test_stats'] = lv_sp_normal_test_stats\n",
    "\n",
    "        if lv_sp_difficult_test_file:\n",
    "            lv_sp_dif_test_stats = evaluate(\n",
    "                net=model,\n",
    "                dataloader=lv_sp_dif_loader,\n",
    "                type='Test Lv Sp Difficult',\n",
    "                epoch=epoch\n",
    "            )\n",
    "            lv_sp_dif_test_stats = {k: _convert_scalar(v) for k, v in lv_sp_dif_test_stats.items() if k in checkpoint_stats_k}\n",
    "            all_test_stats['lv_sp_dif_test_stats'] = lv_sp_dif_test_stats\n",
    "            \n",
    "\n",
    "        save_checkpoint(test_stats['map'], model, optimizer, epoch, False, all_test_stats)\n",
    "\n",
    "        # Save checkpoint stats in json format for ease of viewing\n",
    "        save_checkpoint_stats_json(all_test_stats, epoch, index=False)\n",
    "\n",
    "    if WANDB:\n",
    "        wandb.log({'train_loss': train_stats['train_loss'], 'val_loss': val_eval['loss']}, step=epoch)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
